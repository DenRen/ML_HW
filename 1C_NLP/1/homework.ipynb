{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Hint:\n",
    "#   I'm use `sys.executable` because python in venv on Windows 10 (Yeap...)\n",
    "\n",
    "print(f'python: {sys.executable}')\n",
    "!{sys.executable} -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -qq --upgrade pip\n",
    "!{sys.executable} -m pip install -qq numpy==1.24.1 datasets nltk matplotlib\n",
    "!{sys.executable} -m pip install -qq torch -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('santhosh/english-malayalam-names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_en = dataset['train']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_uniq_freq_names(names: List[str]):\n",
    "    # Select unique names\n",
    "    names_counter = Counter()\n",
    "    for name_and_other in names:\n",
    "        words = name_and_other.split()\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "\n",
    "        words = words[0].split('.')\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "\n",
    "        names_counter.update(words[:1])\n",
    "\n",
    "    return [name for name, freq in names_counter.items() if freq > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, lines: List[str]):\n",
    "        self.id2token, self.token2id = self.train(lines)\n",
    "\n",
    "        self.bos = self.id2token[-4]\n",
    "        self.eos = self.id2token[-3]\n",
    "        self.unk_id = len(self.id2token) - 1\n",
    "        self.pad_id = len(self.id2token) - 2\n",
    "        self.eos_id = len(self.id2token) - 3\n",
    "        self.bos_id = len(self.id2token) - 4\n",
    "\n",
    "    def train(self, uniq_names: List[str]):\n",
    "        tokens_counter = Counter()\n",
    "        for name in uniq_names:\n",
    "            tokens_counter.update(name)  # Names -> symbols\n",
    "\n",
    "        id2token = [token for token in tokens_counter.keys()]\n",
    "        id2token.extend(['<bos>', '<eos>', '<pad>', '<unk>'])\n",
    "        token2id = {token: i for i, token in enumerate(id2token)}\n",
    "\n",
    "        for i in range(len(id2token)):\n",
    "            assert token2id[id2token[i]] == i\n",
    "\n",
    "        return id2token, token2id\n",
    "\n",
    "    def as_matrix(self, x: List[str]) -> torch.Tensor:\n",
    "        tokens = [\n",
    "            torch.LongTensor(\n",
    "                [self.bos_id] +\n",
    "                list(\n",
    "                    map(lambda tt: self.token2id.get(tt, self.unk_id), t)\n",
    "                ) +\n",
    "                [self.eos_id]\n",
    "            ) for t in x]\n",
    "        return pad_sequence(tokens, batch_first=True, padding_value=self.pad_id)\n",
    "\n",
    "    def as_bos_and_indexes(self, x: str) -> torch.Tensor:\n",
    "        return torch.LongTensor([self.bos_id] + list(\n",
    "            map(lambda tt: self.token2id.get(tt, self.unk_id), x)\n",
    "        ))\n",
    "\n",
    "    def decode(self, x: torch.Tensor) -> List[str]:\n",
    "        assert x.ndim == 2, 'pass batch of ids'\n",
    "\n",
    "        tokens = x.tolist()\n",
    "        lines = [self._decode_line(t) for t in tokens]\n",
    "        return lines\n",
    "\n",
    "    def _decode_line(self, tokens: List[int]) -> str:\n",
    "        answer = \"\"\n",
    "        for token_id in tokens:\n",
    "            answer += f'{self.id2token[token_id]} ' if token_id != self.pad_id else ''\n",
    "        return answer.strip()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ClassifierOutput = namedtuple('ClassifierOutput', ['loss', 'logits'])\n",
    "\n",
    "\n",
    "class GenNetwork(torch.nn.Module):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=len(tokenizer),\n",
    "            embedding_dim=32,\n",
    "        )\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size=32,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier_head = torch.nn.Linear(64, len(tokenizer))\n",
    "        self.cross = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "\n",
    "    def _compute_lm_loss(self, predictions: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        predictions = predictions[:, :-1, :].contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        return self.cross(predictions.view(-1, predictions.size(2)), labels.view(-1))\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        emb = self.embeddings(input_ids)\n",
    "        lstm_res, _ = self.gru(emb)\n",
    "        class_res = self.classifier_head(lstm_res)\n",
    "\n",
    "        if labels is None:\n",
    "            output = ClassifierOutput(loss=None, logits=class_res)\n",
    "        else:\n",
    "            loss = self._compute_lm_loss(class_res, labels)\n",
    "            output = ClassifierOutput(loss=loss, logits=class_res)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(vocab, x_data, shuffle: bool = False, batch_size: int = 32, device=device):\n",
    "    x_data = np.array(x_data)\n",
    "\n",
    "    indices = np.arange(len(x_data))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        batch_x = x_data[batch_idx]\n",
    "\n",
    "        input_ids = vocab.as_matrix(batch_x).to(device)\n",
    "        yield {'input_ids': input_ids, 'labels': input_ids}\n",
    "\n",
    "# https://stackoverflow.com/questions/5283649/plot-smooth-line-with-pyplot\n",
    "def smooth(scalars: List[float], weight: float) -> List[float]:  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "        \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fast system testing\n",
    "is_low_dataset = False\n",
    "en = long_en[:20000] if is_low_dataset else long_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_en_names = select_uniq_freq_names(en)\n",
    "vocab = Tokenizer(uniq_en_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GenNetwork(vocab).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_visual_enable = True\n",
    "\n",
    "if is_visual_enable:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "batch_size = 64\n",
    "num_epoch = 40\n",
    "\n",
    "if is_visual_enable:\n",
    "    progressbar = tqdm(total=num_epoch * len(uniq_en_names) //\n",
    "                       batch_size, desc='training')\n",
    "net.train()\n",
    "for epoch in range(num_epoch):\n",
    "    it = enumerate(iterate_batches(net.tokenizer, uniq_en_names,\n",
    "                                   batch_size=batch_size, shuffle=True))\n",
    "    for idx, batch in it:\n",
    "        model_out = net(**batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model_out.loss\n",
    "        loss.backward()\n",
    "        history.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        if is_visual_enable:\n",
    "            progressbar.update(1)\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                plt.plot(np.log(smooth(history, 0.95)), label='loss')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_name_greedy_decoding(net, first_sym='', max_len=30, device=device):\n",
    "    name = first_sym\n",
    "    for _ in range(max_len):\n",
    "        name_indexes = net.tokenizer.as_matrix([name]).to(device)\n",
    "        output = net(name_indexes)\n",
    "        logits = output.logits[0][len(name)]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx = torch.argmax(probs).item()\n",
    "        if idx == net.tokenizer.eos_id:\n",
    "            break\n",
    "\n",
    "        symb = net.tokenizer.id2token[idx]\n",
    "        name += symb\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_name_casual_sampling(net, first_sym='', temp=1, max_len=30, device=device):\n",
    "    name = first_sym\n",
    "    for _ in range(max_len):\n",
    "        name_indexes = net.tokenizer.as_matrix([name]).to(device)\n",
    "\n",
    "        output = net(name_indexes)\n",
    "        logits = output.logits[0][len(name)]\n",
    "\n",
    "        probs = F.softmax(logits / temp, dim=-1).detach().cpu().data.numpy()\n",
    "        idx = np.random.choice(len(logits), p=probs)\n",
    "        if idx == net.tokenizer.eos_id:\n",
    "            break\n",
    "\n",
    "        symb = net.tokenizer.id2token[idx]\n",
    "        name += symb\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_name_top_p(net, top_p, first_sym='', temp=1, max_len=30, device=device):\n",
    "    name = first_sym\n",
    "    for _ in range(max_len):\n",
    "        name_indexes = net.tokenizer.as_matrix([name]).to(device)\n",
    "\n",
    "        output = net(name_indexes)\n",
    "        logits = output.logits[0][len(name)]\n",
    "\n",
    "        probs = F.softmax(logits / temp, dim=-1).detach().cpu().data.numpy()\n",
    "        prob_args = np.argsort(-probs)\n",
    "\n",
    "        prob_acc = 0\n",
    "        for last_idx in range(len(prob_args)):\n",
    "            prob_acc += probs[prob_args[last_idx]]\n",
    "            if prob_acc >= top_p:\n",
    "                break\n",
    "        last_idx = min(last_idx + 1, len(prob_args))\n",
    "\n",
    "        probs = probs[prob_args[:last_idx]] / prob_acc\n",
    "        arg_idx = np.random.choice(last_idx, p=probs)\n",
    "        idx = prob_args[arg_idx]\n",
    "        if idx == net.tokenizer.eos_id:\n",
    "            break\n",
    "\n",
    "        symb = net.tokenizer.id2token[idx]\n",
    "        name += symb\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_name_top_k(net, top_k: int, first_sym='', temp=1, max_len=30, device=device):\n",
    "    top_k = min(top_k, len(net.tokenizer))\n",
    "    name = first_sym\n",
    "    for _ in range(max_len):\n",
    "        name_indexes = net.tokenizer.as_matrix([name]).to(device)\n",
    "\n",
    "        output = net(name_indexes)\n",
    "        logits = output.logits[0][len(name)]\n",
    "\n",
    "        probs = F.softmax(logits / temp, dim=-1).detach().cpu().data.numpy()\n",
    "        prob_args = np.argsort(-probs)\n",
    "        probs = probs[prob_args[:top_k]]\n",
    "        probs /= np.sum(probs)\n",
    "\n",
    "        arg_idx = np.random.choice(top_k, p=probs)\n",
    "        idx = prob_args[arg_idx]\n",
    "        if idx == net.tokenizer.eos_id:\n",
    "            break\n",
    "\n",
    "        symb = net.tokenizer.id2token[idx]\n",
    "        name += symb\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_name_greedy_decoding(net, 'A'),\n",
    "          generate_name_greedy_decoding(net, 'Ai'),\n",
    "          generate_name_greedy_decoding(net, 'Lu'),\n",
    "          generate_name_greedy_decoding(net, 'M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_name_casual_sampling(net, first_sym='Muh', temp=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_name_top_p(net, 0.8, first_sym='M', temp=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_name_top_k(net, 20, first_sym='Lucy', temp=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
